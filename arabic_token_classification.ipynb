{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f855f631",
   "metadata": {},
   "source": [
    "# Arabic Named-Entity Recognition (NER) — Assignment\n",
    "\n",
    "This notebook guides you through building an Arabic NER model using the ANERCorp dataset (`asas-ai/ANERCorp`). Fill in the TODO cells to complete the exercise.\n",
    "\n",
    "- **Objective:** Train a token-classification model (NER) that labels tokens with entity tags (e.g., people, locations, organizations).\n",
    "- **Dataset:** `asas-ai/ANERCorp` — contains tokenized Arabic text and tag sequences.\n",
    "- **Typical Labels:** `B-PER`, `I-PER` (person), `B-LOC`, `I-LOC` (location), `B-ORG`, `I-ORG` (organization), and `O` (outside/no entity). Your code should extract the exact label set from the dataset and build `label_list`, `id2label`, and `label2id` mappings.\n",
    "- **Key Steps (what you will implement):**\n",
    "  1. Load the dataset and inspect samples.\n",
    "  2. Convert the provided words into sentence groupings (use `.` `?` `!` as sentence delimiters) before tokenization so sentence boundaries are preserved.\n",
    "  3. Tokenize with a pretrained Arabic tokenizer and align tokenized sub-words with original labels (use `-100` for tokens to ignore in loss).\n",
    "  4. Prepare `tokenized_datasets` and data collator for dynamic padding.\n",
    "  5. Configure and run model training using `AutoModelForTokenClassification` and `Trainer`.\n",
    "  6. Evaluate using `seqeval` (report precision, recall, F1, and accuracy) and run inference with a pipeline.\n",
    "\n",
    "- **Evaluation:** Use the `seqeval` metric (entity-level precision, recall, F1). When aligning predictions and labels, filter out `-100` entries so only real token labels are compared.\n",
    "\n",
    "- **Deliverables:** Completed notebook with working cells for data loading, tokenization/label alignment, training, evaluation, and an inference example. Add short comments explaining choices (e.g., sentence-splitting strategy, tokenizer settings).\n",
    "\n",
    "Good luck — implement each TODO in order and run the cells to verify output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b101905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Install the required packages for Arabic NER with transformers\n",
    "# Required packages: transformers, datasets, seqeval, evaluate, accelerate\n",
    "# Use pip install with -q flag to suppress output\n",
    "\n",
    "!pip install transformers datasets seqeval evaluate accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: List the files in the current directory to explore the workspace\n",
    "# Hint: Use a simple command to display directory contents\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da23007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the ANERCorp dataset and extract label mappings\n",
    "# Steps:\n",
    "# 1. Import required libraries (datasets, numpy)\n",
    "# 2. Load the \"asas-ai/ANERCorp\" dataset using load_dataset()\n",
    "# 3. Inspect the dataset structure - print the splits and a sample entry\n",
    "# 4. Extract unique tags from the training split\n",
    "# 5. Create label_list (sorted), id2label, and label2id mappings\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# dataset = \n",
    "# print(f\"Dataset Split: {dataset}\")\n",
    "# print(f\"Sample Entry: {dataset['train'][0]}\")\n",
    "# unique_tags = \n",
    "# label_list = \n",
    "# id2label = \n",
    "# label2id = \n",
    "# print(f\"\\nLabel List: {label_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa5b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Verify the dataset was loaded correctly\n",
    "# Print the dataframe or dataset summary to inspect the data structure\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load tokenizer and create tokenization function\n",
    "# Steps:\n",
    "# 1. Import AutoTokenizer from transformers\n",
    "# 2. Set model_checkpoint to \"aubmindlab/bert-base-arabertv02\"\n",
    "# 3. Load the tokenizer using AutoTokenizer.from_pretrained()\n",
    "# 4. Create tokenize_and_align_labels function that:\n",
    "#    - Tokenizes the input text (is_split_into_words=True)\n",
    "#    - Maps tokens to their original words\n",
    "#    - Handles special tokens by setting them to -100\n",
    "#    - Aligns labels with sub-word tokens\n",
    "#    - Returns tokenized inputs with labels\n",
    "# 5. Important: Convert words to sentences using punctuation marks \".?!\" as sentence delimiters\n",
    "#    - This helps the model understand sentence boundaries\n",
    "#    - Hint (suggested approach): group `examples['word']` into sentence lists using \".?!\" as end markers, e.g.:\n",
    "#        sentences = []\n",
    "#        current = []\n",
    "#        for w in examples['word']:\n",
    "#            current.append(w)\n",
    "#            if w in ['.', '?', '!'] or (len(w) > 0 and w[-1] in '.?!'):\n",
    "#                sentences.append(current)\n",
    "#                current = []\n",
    "#        if current:\n",
    "#            sentences.append(current)\n",
    "#      Then align `examples['tag']` accordingly to these sentence groups before tokenization.\n",
    "# 6. Apply the function to the entire dataset using dataset.map()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# model_checkpoint = \n",
    "# tokenizer = \n",
    "\n",
    "# def tokenize_and_align_labels(examples):\n",
    "#     # TODO: Implement tokenization and label alignment\n",
    "#     # Hint: Use tokenizer with is_split_into_words=True\n",
    "#     # Handle -100 for special tokens and sub-words\n",
    "#     # Note: Consider punctuation marks \".?!\" when processing sentence boundaries\n",
    "#     pass\n",
    "\n",
    "# tokenized_datasets = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the compute_metrics function for model evaluation\n",
    "# Steps:\n",
    "# 1. Import evaluate and load \"seqeval\" metric\n",
    "# 2. Create compute_metrics function that:\n",
    "#    - Extracts predictions from model outputs using argmax\n",
    "#    - Filters out -100 labels (special tokens and sub-words)\n",
    "#    - Converts prediction and label IDs back to label names\n",
    "#    - Computes seqeval metrics (precision, recall, f1, accuracy)\n",
    "#    - Returns results as a dictionary\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# seqeval = \n",
    "\n",
    "# def compute_metrics(p):\n",
    "#     # TODO: Implement metric computation\n",
    "#     # Hint: Use np.argmax, filter -100 labels, use seqeval.compute()\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e6e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the model and configure training\n",
    "# Steps:\n",
    "# 1. Import AutoModelForTokenClassification, TrainingArguments, Trainer, and DataCollatorForTokenClassification\n",
    "# 2. Load the model using AutoModelForTokenClassification.from_pretrained() with:\n",
    "#    - model_checkpoint\n",
    "#    - num_labels based on label_list length\n",
    "#    - id2label and label2id mappings\n",
    "# 3. Create TrainingArguments with:\n",
    "#    - output directory \"arabert-ner\"\n",
    "#    - evaluation_strategy=\"epoch\"\n",
    "#    - learning_rate=2e-5\n",
    "#    - batch_size=16 (both train and eval)\n",
    "#    - num_train_epochs=3\n",
    "#    - weight_decay=0.01\n",
    "# 4. Create a DataCollatorForTokenClassification for dynamic padding\n",
    "# 5. Initialize the Trainer with model, args, datasets, data_collator, tokenizer, and compute_metrics\n",
    "# 6. Call trainer.train() to start training\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# model = \n",
    "# args = \n",
    "# data_collator = \n",
    "# trainer = \n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b7ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the trained model with inference\n",
    "# Steps:\n",
    "# 1. Import pipeline from transformers\n",
    "# 2. Create an NER pipeline using the trained model and tokenizer\n",
    "# 3. Use aggregation_strategy=\"simple\" to merge sub-tokens back into words\n",
    "# 4. Test the pipeline with an Arabic text sample\n",
    "# 5. Pretty print the results showing entity, label, and confidence score\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# ner_pipeline = \n",
    "# \n",
    "# text = \"أعلن المدير التنفيذي لشركة أبل تيم كوك عن افتتاح فرع جديد في الرياض.\"\n",
    "# results = \n",
    "#\n",
    "# # Pretty print results\n",
    "# for entity in results:\n",
    "#     print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
