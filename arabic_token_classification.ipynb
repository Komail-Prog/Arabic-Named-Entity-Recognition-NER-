{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f855f631",
   "metadata": {},
   "source": [
    "# Arabic Named-Entity Recognition (NER) — Assignment\n",
    "\n",
    "This notebook guides you through building an Arabic NER model using the ANERCorp dataset (`asas-ai/ANERCorp`). Fill in the TODO cells to complete the exercise.\n",
    "\n",
    "- **Objective:** Train a token-classification model (NER) that labels tokens with entity tags (e.g., people, locations, organizations).\n",
    "- **Dataset:** `asas-ai/ANERCorp` — contains tokenized Arabic text and tag sequences.\n",
    "- **Typical Labels:** `B-PER`, `I-PER` (person), `B-LOC`, `I-LOC` (location), `B-ORG`, `I-ORG` (organization), and `O` (outside/no entity). Your code should extract the exact label set from the dataset and build `label_list`, `id2label`, and `label2id` mappings.\n",
    "- **Key Steps (what you will implement):**\n",
    "  1. Load the dataset and inspect samples.\n",
    "  2. Convert the provided words into sentence groupings (use `.` `?` `!` as sentence delimiters) before tokenization so sentence boundaries are preserved.\n",
    "  3. Tokenize with a pretrained Arabic tokenizer and align tokenized sub-words with original labels (use `-100` for tokens to ignore in loss).\n",
    "  4. Prepare `tokenized_datasets` and data collator for dynamic padding.\n",
    "  5. Configure and run model training using `AutoModelForTokenClassification` and `Trainer`.\n",
    "  6. Evaluate using `seqeval` (report precision, recall, F1, and accuracy) and run inference with a pipeline.\n",
    "\n",
    "- **Evaluation:** Use the `seqeval` metric (entity-level precision, recall, F1). When aligning predictions and labels, filter out `-100` entries so only real token labels are compared.\n",
    "\n",
    "- **Deliverables:** Completed notebook with working cells for data loading, tokenization/label alignment, training, evaluation, and an inference example. Add short comments explaining choices (e.g., sentence-splitting strategy, tokenizer settings).\n",
    "\n",
    "Good luck — implement each TODO in order and run the cells to verify output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b101905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Install the required packages for Arabic NER with transformers\n",
    "# Required packages: transformers, datasets, seqeval, evaluate, accelerate\n",
    "# Use pip install with -q flag to suppress output\n",
    "\n",
    "!pip install transformers datasets seqeval evaluate accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4abb573c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabert-ner  sample_data\n"
     ]
    }
   ],
   "source": [
    "# TODO: List the files in the current directory to explore the workspace\n",
    "# Hint: Use a simple command to display directory contents\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da23007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Split: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['word', 'tag'],\n",
      "        num_rows: 125102\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['word', 'tag'],\n",
      "        num_rows: 25008\n",
      "    })\n",
      "})\n",
      "Sample Entry: {'word': 'فرانكفورت', 'tag': 'B-LOC'}\n",
      "\n",
      "Label List: ['-', 'B', 'C', 'E', 'G', 'I', 'L', 'M', 'O', 'P', 'R', 'S']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load the ANERCorp dataset and extract label mappings\n",
    "# Steps:\n",
    "# 1. Import required libraries (datasets, numpy)\n",
    "# 2. Load the \"asas-ai/ANERCorp\" dataset using load_dataset()\n",
    "# 3. Inspect the dataset structure - print the splits and a sample entry\n",
    "# 4. Extract unique tags from the training split\n",
    "# 5. Create label_list (sorted), id2label, and label2id mappings\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# TODO: Load the ANERCorp dataset and extract label mappings\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dataset = datasets.load_dataset(\"asas-ai/ANERCorp\")\n",
    "\n",
    "\n",
    "print(f\"Dataset Split: {dataset}\")\n",
    "print(f\"Sample Entry: {dataset['train'][0]}\")\n",
    "\n",
    "\n",
    "raw_tags = dataset[\"train\"][\"tag\"]\n",
    "\n",
    "unique_tag_ids = sorted(list(set([t for sublist in raw_tags for t in sublist])))\n",
    "\n",
    "\n",
    "label_list = [str(t) for t in unique_tag_ids] \n",
    "\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "print(f\"\\nLabel List: {label_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa5b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Summary:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['word', 'tag'],\n",
      "        num_rows: 125102\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['word', 'tag'],\n",
      "        num_rows: 25008\n",
      "    })\n",
      "})\n",
      "\n",
      "First 5 entries (Sentence lists):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tag",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "808b22eb-20c8-48c6-9c5a-7058527e68bf",
       "rows": [
        [
         "0",
         "فرانكفورت",
         "B-LOC"
        ],
        [
         "1",
         "(د",
         "O"
        ],
        [
         "2",
         "ب",
         "O"
        ],
        [
         "3",
         "أ)",
         "O"
        ],
        [
         "4",
         "أعلن",
         "O"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "\n",
       "  <div id=\"df-312d00cc-2d90-4f64-8357-64bd17bb8dd9\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>فرانكفورت</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(د</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ب</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>أ)</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>أعلن</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-312d00cc-2d90-4f64-8357-64bd17bb8dd9')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-312d00cc-2d90-4f64-8357-64bd17bb8dd9 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-312d00cc-2d90-4f64-8357-64bd17bb8dd9');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "        word    tag\n",
       "0  فرانكفورت  B-LOC\n",
       "1         (د      O\n",
       "2          ب      O\n",
       "3         أ)      O\n",
       "4       أعلن      O"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Features:\n",
      "{'word': Value('string'), 'tag': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Verify the dataset was loaded correctly\n",
    "import pandas as pd  \n",
    "\n",
    "\n",
    "print(\"Dataset Summary:\")\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "df_sample = pd.DataFrame(dataset[\"train\"][:5])\n",
    "print(\"\\nFirst 5 entries (Sentence lists):\")\n",
    "display(df_sample)\n",
    "\n",
    "\n",
    "print(\"\\nDataset Features:\")\n",
    "print(dataset[\"train\"].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78b0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35956010cd45400fa1f39b66f263adb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4317 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b5980514104f1aba0ceffa94b4dfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/987 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully tokenized!\n",
      "Sample labels: [-100, 1, 0, 6, 8, 2, 8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load tokenizer and create tokenization function\n",
    "# Steps:\n",
    "# 1. Import AutoTokenizer from transformers\n",
    "# 2. Set model_checkpoint to \"aubmindlab/bert-base-arabertv02\"\n",
    "# 3. Load the tokenizer using AutoTokenizer.from_pretrained()\n",
    "# 4. Create tokenize_and_align_labels function that:\n",
    "#    - Tokenizes the input text (is_split_into_words=True)\n",
    "#    - Maps tokens to their original words\n",
    "#    - Handles special tokens by setting them to -100\n",
    "#    - Aligns labels with sub-word tokens\n",
    "#    - Returns tokenized inputs with labels\n",
    "# 5. Important: Convert words to sentences using punctuation marks \".?!\" as sentence delimiters\n",
    "#    - This helps the model understand sentence boundaries\n",
    "#    - Hint (suggested approach): group `examples['word']` into sentence lists using \".?!\" as end markers, e.g.:\n",
    "#        sentences = []\n",
    "#        current = []\n",
    "#        for w in examples['word']:\n",
    "#            current.append(w)\n",
    "#            if w in ['.', '?', '!'] or (len(w) > 0 and w[-1] in '.?!'):\n",
    "#                sentences.append(current)\n",
    "#                current = []\n",
    "#        if current:\n",
    "#            sentences.append(current)\n",
    "#      Then align `examples['tag']` accordingly to these sentence groups before tokenization.\n",
    "# 6. Apply the function to the entire dataset using dataset.map()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# YOUR CODE HERE\n",
    "def group_into_sentences(examples):\n",
    "    all_sentences = []\n",
    "    all_tags = []\n",
    "    current_sentence = []\n",
    "    current_tags = []\n",
    "    \n",
    "    for words, tags in zip(examples[\"word\"], examples[\"tag\"]):\n",
    "        for w, t in zip(words, tags):\n",
    "            current_sentence.append(w)\n",
    "\n",
    "            if isinstance(t, str):\n",
    "                current_tags.append(label2id[t])\n",
    "            else:\n",
    "                current_tags.append(int(t))\n",
    "                \n",
    "            if w in ['.', '?', '!'] or (len(w) > 0 and w[-1] in '.?!'):\n",
    "                all_sentences.append(current_sentence)\n",
    "                all_tags.append(current_tags)\n",
    "                current_sentence = []\n",
    "                current_tags = []\n",
    "    \n",
    "    if current_sentence:\n",
    "        all_sentences.append(current_sentence)\n",
    "        all_tags.append(current_tags)\n",
    "        \n",
    "    return {\"tokens\": all_sentences, \"ner_tags\": all_tags}\n",
    "\n",
    "\n",
    "prepared_dataset = dataset.map(\n",
    "    group_into_sentences, \n",
    "    batched=True, \n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "\n",
    "tokenized_datasets = prepared_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "\n",
    "print(\"Successfully tokenized!\")\n",
    "print(f\"Sample labels: {tokenized_datasets['train'][0]['labels'][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the compute_metrics function for model evaluation\n",
    "# Steps:\n",
    "# 1. Import evaluate and load \"seqeval\" metric\n",
    "# 2. Create compute_metrics function that:\n",
    "#    - Extracts predictions from model outputs using argmax\n",
    "#    - Filters out -100 labels (special tokens and sub-words)\n",
    "#    - Converts prediction and label IDs back to label names\n",
    "#    - Computes seqeval metrics (precision, recall, f1, accuracy)\n",
    "#    - Returns results as a dictionary\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# YOUR CODE HERE\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    \n",
    "\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e6e403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-1683120635.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='810' max='810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [810/810 02:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.777200</td>\n",
       "      <td>0.820312</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>0.162602</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.725248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.588600</td>\n",
       "      <td>0.763315</td>\n",
       "      <td>0.323308</td>\n",
       "      <td>0.227236</td>\n",
       "      <td>0.266889</td>\n",
       "      <td>0.751629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.505200</td>\n",
       "      <td>0.725800</td>\n",
       "      <td>0.346010</td>\n",
       "      <td>0.262602</td>\n",
       "      <td>0.298590</td>\n",
       "      <td>0.757721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: - seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: R seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: G seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: M seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: - seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: R seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: G seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: M seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: - seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: L seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: P seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: R seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: G seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.12/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: M seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=810, training_loss=0.6542675618772154, metrics={'train_runtime': 144.3377, 'train_samples_per_second': 89.727, 'train_steps_per_second': 5.612, 'total_flos': 627941135138184.0, 'train_loss': 0.6542675618772154, 'epoch': 3.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load the model and configure training\n",
    "# Steps:\n",
    "# 1. Import AutoModelForTokenClassification, TrainingArguments, Trainer, and DataCollatorForTokenClassification\n",
    "# 2. Load the model using AutoModelForTokenClassification.from_pretrained() with:\n",
    "#    - model_checkpoint\n",
    "#    - num_labels based on label_list length\n",
    "#    - id2label and label2id mappings\n",
    "# 3. Create TrainingArguments with:\n",
    "#    - output directory \"arabert-ner\"\n",
    "#    - evaluation_strategy=\"epoch\"\n",
    "#    - learning_rate=2e-5\n",
    "#    - batch_size=16 (both train and eval)\n",
    "#    - num_train_epochs=3\n",
    "#    - weight_decay=0.01\n",
    "# 4. Create a DataCollatorForTokenClassification for dynamic padding\n",
    "# 5. Initialize the Trainer with model, args, datasets, data_collator, tokenizer, and compute_metrics\n",
    "# 6. Call trainer.train() to start training\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    ")\n",
    "args = TrainingArguments(\n",
    "    \"arabert-ner\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,               \n",
    "    per_device_train_batch_size=8,    \n",
    "    per_device_eval_batch_size=8,     \n",
    "    gradient_accumulation_steps=2,   \n",
    "    num_train_epochs=3,               \n",
    "    weight_decay=0.01,                \n",
    "    logging_steps=100,                \n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=True                         \n",
    ")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"], \n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "496b7ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: أعلن المدير التنفيذي لشركة أبل تيم كوك عن افتتاح فرع جديد في الرياض.\n",
      "\n",
      "------------------------------\n",
      "Entity: لشركة, Label: , Score: 0.26\n",
      "Entity: تيم, Label: , Score: 0.19\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test the trained model with inference\n",
    "# Steps:\n",
    "# 1. Import pipeline from transformers\n",
    "# 2. Create an NER pipeline using the trained model and tokenizer\n",
    "# 3. Use aggregation_strategy=\"simple\" to merge sub-tokens back into words\n",
    "# 4. Test the pipeline with an Arabic text sample\n",
    "# 5. Pretty print the results showing entity, label, and confidence score\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "text = \"أعلن المدير التنفيذي لشركة أبل تيم كوك عن افتتاح فرع جديد في الرياض.\"\n",
    "results = ner_pipeline(text)\n",
    "\n",
    "print(f\"Input Text: {text}\\n\")\n",
    "print(\"-\" * 30)\n",
    "for entity in results:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
